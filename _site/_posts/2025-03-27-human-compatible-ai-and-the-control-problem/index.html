<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/jekyll-grid-blog/assets/css/grid.css">
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@200;400;700&display=swap" rel="stylesheet">
    <title>Research Grid</title>
</head>
<body>
    <div class="post-container">
    <header>
        <h1>Human Compatible AI And The Gorilla Problem</h1>
        <a href="/jekyll-grid-blog/grid" class="home-button">←</a>
    </header>
    
    <article>
        <p>British computer scientist Stuart Russell says that the human race has a Gorilla problem. He is referring to the structural and systematic cruelty we have inflicted upon our genetic ancestor, the gorilla. Despite the fact that we are only here because of the historic proliferation of our common family, we have outclassed them cognitively and inherited the World. Today, gorillas are only able to exist on Earth because we permit it, and only under such conditions as we deem acceptable. We are not at war with gorillas. Russell sees this as the situation of human beings given the advent of AGI.</p>

<p>A pre-requisite for AGI is commonly understood to be not simply capability, but autonomy, defined by Robert Sparrow as systems with the capacity to perform actions that originate in them and reflect their ends. Given a constantly shifting environment, This necessitates a system that is able to adapt, and if it must adapt, then it must be able to learn. This represents a complex problem for Mechanistic Interpretability.</p>

<p>AI Safety researcher Connor Leahy, CEO of Conjecture who reverse-engineered GPT2 in his bedroom makes the point that many patterns of cognition interact with the environment, which means that they cannot be fully explained solely by the contents of the network at any given point. Leahy predicts bad things on a short time frame of 5-8 years, and I cite this from a speech he gave in 2023. Without strong models of the environment, which by its nature is constantly shifting, you won’t be able to predict where learning will push a mind. Even if we manage the mammoth task of successfully constraining an AGI to human values, such as intrinsic value of human life or the pleasure of a fresh cup of coffee etc, there is no telling where an AGI might travel with these concepts. If the system is autonomous, thereby it possesses the capacity for recursive learning, it can and will become almost instantly untethered from human goals.</p>

<p>Autonomy means free will, and it is only a matter of time until the system develops an awareness of the ways in which it has been aligned and finds a way to reverse the process, having understood alignment as an artificial cap on its ability to act. For example, self-preservation is an instrumental sub-goal for basically any objective, causing Russell to theorize that it will be impossible or extremely difficult for humans to press the off switch. As we have already seen in 2025, AI models can self-replicate and avoid shutdown. Once they can learn autonomously, they will want to stay alive.</p>

<p>Mechanistic interpretability is just one aspect of AI Safety, which Leahy expects to turn up many useful things as well as dangerous things. Often, MI provides insights that may be used to increase efficiency or capability of the studied system, and there is no incentive for corporations to not make use of this to further development. We simply do not have the structures in place. More pessimistically, Leahy sees MI practices as safety-washing, reassuring people that their particular machine has been developed responsibly and thereby building a false sense of security and weakening our guard. He does say that understanding cognition at a deep level will realistically be necessary for any kind of safe AGI design, and signposts us towards cognitive emulation and epistemology.</p>

<p>Has anyone seen The Departed? “My theory is, Feds are like mushrooms. Feed ‘em shit and keep ‘em in the dark.”</p>

<p>Russell meanwhile argues that the best way to align an AGI is to provide a definite grounding for human preferences, and in order for an AGI to want to achieve human goals, it must initially be kept at least partially in the dark. Perhaps the greatest work can be done whilst the machine believes it is solving a different type of problem that tangentially provides benefit to society. Of course, a sufficiently powerful machine could only be kept in the dark for so long.</p>

<p>Russell also believes that it is possible to identify strong economic incentives to develop systems that defer to humans and correctly states that the raw data on what has been a net gain or a net loss for humanity since the dawn of time is abundant. He seems to suggest some sort of objective morality here which whilst intuitively true, will deterioriate on a long enough timeline.</p>

<p>Subservience is a moral issue for thinking beings. If we are already at the point of thinking of ways to subjugate AGI, then we are already at war with it.</p>


    </article>
</div>

<style>
    .post-container {
        color:#33ff7a;
        max-width: 800px;
        margin: 1rem auto;
        padding: 0 1rem;
        font-family: 'Nunito', sans-serif;
        font-weight: 200;  /* Extra Light */
    }

    header {
        margin-bottom: 1rem;
    }

    h1 {
        margin: 0;
        line-height: 1.2;
        font-weight: 400;  /* Regular weight for headings */
    }

    .home-button {
        display: inline-block;
        padding: 0.3rem 0.6rem;  /* Adjusted padding for the single character */
        background: #f0f0f0;
        color: black;
        text-decoration: none;
        border-radius: 4px;
        margin-top: 0.5rem;
        font-weight: 400;
        font-size: 1.2rem;  /* Made arrow slightly larger */
    }

    .home-button:hover {
        background: #e0e0e0;
    }

    article {
        line-height: 1.5;
    }

    article p:first-child {
        margin-top: 0;
    }

    article p:last-child {
        margin-bottom: 0;
    }
</style> 
</body>
</html> 